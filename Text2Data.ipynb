{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# **Step 1: Dataset Generation**\n",
        "# 1.1: Identify Date Formats\n",
        "informal_dates = [\n",
        "    \"1 1403 Mehr\",                # Informal date\n",
        "    \"First Mehr 1403\",            # Informal date\n",
        "    \"Mehr 1403\",                  # Informal date\n",
        "    \"Mehr month 1403\",            # Informal date\n",
        "    \"5 Aban 1403\",                # Informal date with day\n",
        "    \"25 Azar 1403\",               # Informal date with day\n",
        "    \"30 Tir 1403\",                # Informal date with day\n",
        "    \"Thursday 5 Aban 1403\",       # Informal date with day\n",
        "    \"1 Farvardin 1403\",           # Informal date\n",
        "    \"Last day of Tir 1403\",       # Informal date\n",
        "    \"14 Dey 1403\",                # Informal date with day\n",
        "    \"8 Bahman 1403\",              # Informal date with day\n",
        "    \"1403/07/01\",                  # Formal date\n",
        "    \"1403-07-01\",                  # Formal date\n",
        "    \"1403/08/05\",                  # Formal date with day\n",
        "    \"1403/09/25\"                   # Formal date with day\n",
        "]\n",
        "\n",
        "# Corresponding formal formats\n",
        "formal_dates = [\n",
        "    \"1403/07/01\",                  # Formal date\n",
        "    \"1403/07/01\",                  # Formal date\n",
        "    \"1403/07/01\",                  # Formal date\n",
        "    \"1403/07/01\",                  # Formal date\n",
        "    \"1403/08/05\",                  # Formal date with day\n",
        "    \"1403/09/25\",                  # Formal date with day\n",
        "    \"1403/04/30\",                  # Formal date with day\n",
        "    \"1403/08/05\",                  # Formal date with day\n",
        "    \"1403/01/01\",                  # Formal date\n",
        "    \"1403/04/30\",                  # Formal date\n",
        "    \"1403/10/14\",                  # Formal date with day\n",
        "    \"1403/11/08\",                  # Formal date with day\n",
        "    \"1403/07/01\",                  # Formal date\n",
        "    \"1403/07/01\",                  # Formal date\n",
        "    \"1403/08/05\",                  # Formal date with day\n",
        "    \"1403/09/25\"                   # Formal date with day\n",
        "]\n",
        "\n",
        "# 1.3: Create Dataset\n",
        "data = {\n",
        "    \"informal_date\": informal_dates,\n",
        "    \"formal_date\": formal_dates\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 1.4: Save and Display Dataset\n",
        "df.to_csv('date_conversion_dataset.csv', index=False)\n",
        "print(df)\n",
        "\n",
        "# **Step 2: Data Preparation**\n",
        "# Load the dataset\n",
        "df = pd.read_csv('date_conversion_dataset.csv')\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the T5 tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings = tokenizer(train_data['informal_date'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
        "train_labels = tokenizer(train_data['formal_date'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
        "test_encodings = tokenizer(test_data['informal_date'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
        "test_labels = tokenizer(test_data['formal_date'].tolist(), truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "# **Step 3: Model Training**\n",
        "# Create data loaders\n",
        "train_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'], train_labels['input_ids'])\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Training settings\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "model.train()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(5):  # Increase the number of epochs for better learning\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, labels = batch\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# **Step 4: Model Evaluation**\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "test_encodings = test_encodings['input_ids']\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(test_encodings)\n",
        "\n",
        "# Decode tokens to text\n",
        "decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# Function to ensure correct year, month, and format in predictions\n",
        "def correct_date_format(predicted, informal):\n",
        "    # Define month keywords and their corresponding numbers\n",
        "    month_keywords = {\n",
        "        \"Farvardin\": \"01\",\n",
        "        \"Ordibehesht\": \"02\",\n",
        "        \"Khordad\": \"03\",\n",
        "        \"Tir\": \"04\",\n",
        "        \"Mordad\": \"05\",\n",
        "        \"Shahrivar\": \"06\",\n",
        "        \"Mehr\": \"07\",\n",
        "        \"Aban\": \"08\",\n",
        "        \"Azar\": \"09\",\n",
        "        \"Dey\": \"10\",\n",
        "        \"Bahman\": \"11\",\n",
        "        \"Esfand\": \"12\"\n",
        "    }\n",
        "    # Extract year from the informal date\n",
        "    year = None\n",
        "    for part in informal.split():\n",
        "        if part.isdigit() and len(part) == 4:  # Check if it's a year\n",
        "            year = part\n",
        "    # Find the month from informal input\n",
        "    month_number = None\n",
        "    for month_name, month_num in month_keywords.items():\n",
        "        if month_name in informal:\n",
        "            month_number = month_num\n",
        "            break\n",
        "    # Extract day from the predicted output\n",
        "    parts = predicted.split('/')\n",
        "    if len(parts) == 3:\n",
        "        day = parts[1]  # Use the day part from predicted\n",
        "    else:\n",
        "        day = '01'  # Default day if not found\n",
        "    # If year is found, construct the final date\n",
        "    if year and month_number:\n",
        "        return f\"{year}/{month_number}/{day.zfill(2)}\"\n",
        "    return predicted  # Return the original if no month or year is found\n",
        "\n",
        "# Display results\n",
        "for informal, formal, decoded in zip(test_data['informal_date'], test_data['formal_date'], decoded_outputs):\n",
        "    corrected_decoded = correct_date_format(decoded, informal)\n",
        "    print(f'Informal: {informal}, Expected: {formal}, Predicted: {corrected_decoded}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i_ulH1uy3Iu",
        "outputId": "c92140d3-fcef-413a-b7b7-08b81a44faa7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           informal_date formal_date\n",
            "0            1 1403 Mehr  1403/07/01\n",
            "1        First Mehr 1403  1403/07/01\n",
            "2              Mehr 1403  1403/07/01\n",
            "3        Mehr month 1403  1403/07/01\n",
            "4            5 Aban 1403  1403/08/05\n",
            "5           25 Azar 1403  1403/09/25\n",
            "6            30 Tir 1403  1403/04/30\n",
            "7   Thursday 5 Aban 1403  1403/08/05\n",
            "8       1 Farvardin 1403  1403/01/01\n",
            "9   Last day of Tir 1403  1403/04/30\n",
            "10           14 Dey 1403  1403/10/14\n",
            "11         8 Bahman 1403  1403/11/08\n",
            "12            1403/07/01  1403/07/01\n",
            "13            1403-07-01  1403/07/01\n",
            "14            1403/08/05  1403/08/05\n",
            "15            1403/09/25  1403/09/25\n",
            "Epoch: 0, Loss: 5.472968578338623\n",
            "Epoch: 0, Loss: 8.722607612609863\n",
            "Epoch: 0, Loss: 13.696084022521973\n",
            "Epoch: 0, Loss: 3.920201539993286\n",
            "Epoch: 0, Loss: 12.099652290344238\n",
            "Epoch: 0, Loss: 5.620405673980713\n",
            "Epoch: 1, Loss: 4.868524074554443\n",
            "Epoch: 1, Loss: 8.467852592468262\n",
            "Epoch: 1, Loss: 7.704671859741211\n",
            "Epoch: 1, Loss: 7.730161666870117\n",
            "Epoch: 1, Loss: 4.705392837524414\n",
            "Epoch: 1, Loss: 6.17329740524292\n",
            "Epoch: 2, Loss: 4.559884548187256\n",
            "Epoch: 2, Loss: 6.868271350860596\n",
            "Epoch: 2, Loss: 4.5116143226623535\n",
            "Epoch: 2, Loss: 3.6763527393341064\n",
            "Epoch: 2, Loss: 2.1989033222198486\n",
            "Epoch: 2, Loss: 6.876256465911865\n",
            "Epoch: 3, Loss: 2.8452088832855225\n",
            "Epoch: 3, Loss: 3.839630126953125\n",
            "Epoch: 3, Loss: 5.139140605926514\n",
            "Epoch: 3, Loss: 4.980079650878906\n",
            "Epoch: 3, Loss: 4.57682991027832\n",
            "Epoch: 3, Loss: 4.989480972290039\n",
            "Epoch: 4, Loss: 6.012096881866455\n",
            "Epoch: 4, Loss: 3.638528823852539\n",
            "Epoch: 4, Loss: 4.4680562019348145\n",
            "Epoch: 4, Loss: 4.396841049194336\n",
            "Epoch: 4, Loss: 3.8054306507110596\n",
            "Epoch: 4, Loss: 4.537078857421875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Informal: 1 1403 Mehr, Expected: 1403/07/01, Predicted: 1403/07/01\n",
            "Informal: First Mehr 1403, Expected: 1403/07/01, Predicted: 1403/07/01\n",
            "Informal: 25 Azar 1403, Expected: 1403/09/25, Predicted: 1403/09/01\n",
            "Informal: 1403/08/05, Expected: 1403/08/05, Predicted: 1403/08/05\n"
          ]
        }
      ]
    }
  ]
}